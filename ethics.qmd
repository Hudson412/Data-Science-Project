---
title: "Ethical Analysis of Amazon's AI Reruiting Tool"
bibliography: references.bib
---

In 2014, Amazon developed a recruiting tool that used artificial intelligence to scan resumes in order to increase the efficiency of the hiring process for positions at the company. This tool was originally intended to automatically and quickly identify top candidates from a pool by learning patters from historical hiring data. Eventually, however, Amazon realized that the algorithm was actually quite unfair. It was consistently downgrading resumes that included the word "women" or came from an individual attending a women's college. The tech giant ultimately traced the bias back to the original data that the model was based off of. The data came from the past 10 years of hiring data, which mainly consisted of male applicants. During this period, there was a national trend where the majority of positions in the tech industry were filled by men. The model taught itself that male candidates were preferable for the company, reflecting the male dominance in the field and Amazon's historical applicant pool. Thus, the tool led to discriminating against female candidates, and the firm discontinued use of this artificial intelligence tool for recruitment. This occurrence as a whole presents a significant ethical dilemma. [@Dastin2018]

Others argue that Amazon was not completely at fault for this hiring flaw as bias in AI can be driven by a multitude of factors. There are many factors unrelated to the data that the tool pulls from that could have caused this issue. The individuals that use the tool along with other external variables can adjust how artificial intelligence "feels" towards something. [@Lavanchy2018]

When companies collect data from job applicants, there is an expectation that the information will be used solely for evaluating their candidacy. Speaking to the permission structure for using the data in this situation, the resumes used to train Amazon's hiring tool were originally submitted for the purpose of obtaining employment at the company. Job applicants were sharing their personal information, in the form of a resume, under the impression that only job recruiters were the only individuals using this information. There is no evidence that the applicants were explicitly informed that the data would actually be used to train an artificial intelligence system. And this goes for individuals from a period of ten years that applied to Amazon. Thus, it is unclear whether or not the permission structure of using the data was not correctly followed, as there is no evidence that applicants were informed their resumes would be used to train an AI system. Presumably, most candidates submitted their information with the expectation that they would be reviewed for the sole purpose of employment consideration. If the company had explicitly specified that resumes would be used for their typical usage and model training, then the permission structure would have been followed a bit closer. [@Dastin2018]

For an algorithm to make fair decisions, the data it is trained on must accurately represent the population it will be applied to. One of if not the largest flaws in this situation is who was measured to train the algorithm. As stated in the overview of the event, the hiring tool was trained on resumes sent in to Amazon over a ten year period. This information was primarily submitted for technical roles at the company. This period stands in the middle of the tech industry's extreme gender imbalance [@Lavanchy2018]. For many years, jobs within the tech world have been dominated by males. Thus, the applicant pool that Amazon pulled its resumes from was extremely skewed toward men. For this reason, the algorithm quickly began to associate male characteristics and applicants with successful candidates (those who were hired in the past). The usage of this model would only reaffirm and amplify existing biases in the field. Using this dataset to generalize or apply the algorithm to all applicants for the position at Amazon is unethical and results in biased outcomes. We must raise concerns when analyzing data that we do not know how it was collected. [@Dastin2018]

Identifiable data is information that can be directly traced back to a specific individual. A few examples would include things like names, contact information, and other unique identifiers. In order to protect the privacy of customers, companies will typically try to keep this information secure or anonymized by removing identifiers. In this situation, Amazon was working with resumes, which contain lots of personal identifiable information like names, education, and work experience. Thus, the information within this data was inherently identifiable. Neither article explicitly mentions whether any anonymization processes were used before the data was used for the algorithm. While there is no evidence of the data being leaked or misused externally, the lack of transparency on Amazon's part is concerning. Since the algorithm learned every detail on the resumes, including identifying details, the risk of identification and unintended use of personal data remained unethically significant. [@Dastin2018]

\
Gender is a complex variable, especially in contexts like hiring where fairness and equal opportunity are critical. Even when gender is not meaningfully included as a data point, learned algorithms can infer an individuals gender. This inference can then lead to biased or discriminatory outcomes if it is not carefully managed. Amazon's AI hiring tool penalized resumes that included the word "women's" or gave reason for the algorithm to believe the applicant was female [@Lavanchy2018]. This is an example of how gender can become a proxy variable in data science even when it is not explicitly included in a study. Thus, using race or gender as variables in artificial intelligence like hiring algorithms is unethical as it can serve as a proxy for continued social disadvantages. Furthermore, these disadvantages would ultimately lead to discrimination and worsened inequalities.
